{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f283aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pyreadr\n",
    "import time\n",
    "import datetime\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.colors as mcolors\n",
    "import statsmodels.api as sm\n",
    "import igraph as ig\n",
    "from folium.plugins import MarkerCluster\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.dates as mdates\n",
    "####################################################\n",
    "# Set working directory\n",
    "os.chdir('C:\\\\Users\\\\Elkanah\\\\Desktop\\\\Elkanah\\\\Spatiotemporal_AR_Model_LASSO')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316b89bc",
   "metadata": {},
   "source": [
    "#  Air Quality Monitoring Stations in Bavaria region, Germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295e9970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of hourly timepoints are 136923\n",
      "The number of monitoring stations are 26\n",
      "The shape of the Y: (26, 136923)\n",
      "The shape of the design matrix X: (26, 136923, 10)\n",
      "The value of k is 10 \n",
      "Full Parameter Estimation has started\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyreadr\n",
    "import time\n",
    "import datetime\n",
    "import folium\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import scipy.sparse as sp\n",
    "from scipy.linalg import solve\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib.colors as mcolors\n",
    "import statsmodels.api as sm\n",
    "import igraph as ig\n",
    "from folium.plugins import MarkerCluster\n",
    "from adjustText import adjust_text\n",
    "import matplotlib.dates as mdates\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed\n",
    "from joblib import Parallel, delayed\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "starttime = pd.Timestamp.now()\n",
    "#Import the cleaned data\n",
    "data = pd.read_csv('PM10_BFF.csv')\n",
    "Y = data.drop('Date', axis=1).values.T\n",
    "data = pd.read_csv('PM10_BFF.csv')\n",
    "dates = pd.to_datetime(data['Date'])\n",
    "Y = data.drop('Date', axis=1).values.T\n",
    "#Y = Y[:, :30000]\n",
    "#########################################\n",
    "Time = Y.shape[1]\n",
    "print('The number of hourly timepoints are',Time)\n",
    "n = Y.shape[0]\n",
    "print('The number of monitoring stations are',n)\n",
    "print('The shape of the Y:', Y.shape) # Create an array to represent the time index\n",
    "time_index = np.arange(Time)\n",
    "\n",
    "#Create the sine and cosine components for temporal patterns with additional frequencies\n",
    "sine_func_yearly = np.sin((2 * np.pi / (365 * 24))*time_index)\n",
    "sine_func_semi_yearly = np.sin((2 * np.pi / (365 * 24/2))*time_index)\n",
    "sine_func_monthly = np.sin((2 * np.pi / (365 * 24/12))*time_index)\n",
    "sine_func_daily = np.sin((2 * np.pi / 24)*time_index)\n",
    "\n",
    "########################################################################\n",
    "cosine_func_yearly = np.cos((2 * np.pi / (365 * 24))* time_index)\n",
    "cosine_func_semi_yearly = np.cos((2 * np.pi / (365 * 24/2))* time_index)\n",
    "cosine_func_monthly = np.cos((2 * np.pi / (365 * 24/12))* time_index)\n",
    "cosine_func_daily = np.cos((2 * np.pi / 24)* time_index)\n",
    "\n",
    "######################################################################\n",
    "sine_mean_func = (sine_func_yearly+sine_func_semi_yearly+sine_func_monthly+sine_func_daily)/5\n",
    "cosine_mean_func =(cosine_func_yearly+cosine_func_semi_yearly+cosine_func_monthly+cosine_func_daily)/5\n",
    "##############################################################################################\n",
    "# Create a design matrix X with sine, cosine, and constant terms\n",
    "#X = np.column_stack((np.ones(Time), sine_mean_func, cosine_mean_func))\n",
    "X = np.column_stack((np.ones(Time),np.linspace(0, 1, Time), sine_func_yearly, sine_func_semi_yearly,sine_func_monthly,sine_func_daily,cosine_func_yearly,cosine_func_semi_yearly,cosine_func_monthly,cosine_func_daily)) # Single-station design matrix\n",
    "X = np.tile(X[np.newaxis, :, :], (n, 1, 1)) #All stations\n",
    "k = X.shape[2]\n",
    "print('The shape of the design matrix X:', X.shape) #### dimension n * Time * 3\n",
    "print(f\"The value of k is {k} \")\n",
    "########################################################################################\n",
    "######################################################################################\n",
    "# Define the Log-Likelihood function\n",
    "def LL(parameters, Y, X, k,n, Time, lambda1, lambda2):\n",
    "    beta = parameters[:k]\n",
    "    phi = parameters[k:k+n]\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, 1)] = parameters[int(k+n):int(k+n+(0.5*n*(n-1)))]\n",
    "    W[np.tril_indices(n, -1)] = parameters[int(k+n+(0.5*n*(n-1))):int(k+(n*n))]\n",
    "    sigma2_eps = parameters[int(k+(n*n))]\n",
    "    \n",
    "     # Calculate sum of squares\n",
    "    residuals_est = np.zeros(Time - 1)\n",
    "    for t in range(1, Time):\n",
    "        u_t = Y[:, t] - W @ Y[:, t] - (phi * Y[:, t-1]) - X[:, t] @ beta\n",
    "        residuals_est[t-1] = np.sum(u_t**2 / sigma2_eps)\n",
    "\n",
    "    sum_of_squares = np.sum(residuals_est)\n",
    "\n",
    "    # Calculate log-likelihood\n",
    "    Constant = -0.5 * (Time - 1) * (np.log(2 * np.pi) + np.sum(n * np.log(sigma2_eps))) + (Time - 1) * np.linalg.slogdet(np.eye(n) - W)[1]\n",
    "    LogLik = Constant - (0.5 * sum_of_squares) - (lambda1 * np.sum(np.abs(W)) + lambda2 * np.sum(np.abs(phi)))\n",
    "    return -LogLik\n",
    "#################################################################################################\n",
    "#################################################################################################\n",
    "def constraint_func(parameters):\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, k=1)] = parameters[int(k+n):int(k+n+0.5*n*(n-1))]\n",
    "    W[np.tril_indices(n, k=-1)] = parameters[int(k+n+0.5*n*(n-1)):int(k+n+n*(n-1))]\n",
    "    row_sums = np.sum(W, axis=1)\n",
    "    return 1 - row_sums# Constraint: row_sums <= 1\n",
    "#################################################################################################\n",
    "# Placeholder for the AIC calculation\n",
    "def calculate_AIC(log_likelihood, num_params):\n",
    "    return 2 * num_params - 2 * log_likelihood\n",
    "\n",
    "# Function to perform the optimization for a given set of lambdas\n",
    "def optimize_for_lambdas(lambda_comb, Y, X, k, n, Time, param, bounds):\n",
    "    lambda1, lambda2 = lambda_comb\n",
    "    result = minimize(LL, param, args=(Y, X, k, n, Time, lambda1, lambda2), bounds=bounds)\n",
    "    log_likelihood = -result.fun\n",
    "    num_params = len(param)\n",
    "    aic = calculate_AIC(log_likelihood, num_params)\n",
    "    return (lambda1, lambda2), aic, result\n",
    "\n",
    "# Generate lambda values\n",
    "lambda_values1 = np.concatenate(([0], np.logspace(-1, 0, 5)))\n",
    "lambda_values2 = np.concatenate(([0], np.logspace(-1, 0, 5)))\n",
    "param_combinations = [(lambda1, lambda2) for lambda1 in lambda_values1 for lambda2 in lambda_values2] # Create parameter combinations for lambda1 and lambda2\n",
    "\n",
    "#Define the bounds for the optimization\n",
    "lb = np.concatenate(([-np.inf]*k, [0]*n, [0]*(int(n*(n-1))), [0.00001]))\n",
    "ub = np.concatenate(([np.inf]*k, [1]*n, [1]*(int(n*(n-1))), [np.inf]))\n",
    "bounds = list(zip(lb, ub))\n",
    "# initialize the parameters\n",
    "param = np.concatenate((np.repeat(0, k), np.repeat(0.01, n), np.repeat(0.001, int(n*(n-1))), [1]))\n",
    "\n",
    "# Function to find the best lambda combination\n",
    "def find_best_lambda(param_combinations, Y, X, k, n, Time, param, bounds):\n",
    "    results = Parallel(n_jobs=-1, backend=\"loky\", verbose=10)(\n",
    "        delayed(optimize_for_lambdas)(comb, Y, X, k, n, Time, param, bounds) for comb in param_combinations)\n",
    "    \n",
    "    best_aic = float('inf')\n",
    "    best_result = None\n",
    "    best_comb = None\n",
    "    for comb, aic, result in results:\n",
    "        if aic < best_aic:\n",
    "            best_aic = aic\n",
    "            best_result = result\n",
    "            best_comb = comb\n",
    "    return best_comb, best_result\n",
    "\n",
    "print(\"Full Parameter Estimation has started\")\n",
    "starttime = pd.Timestamp.now()\n",
    "\n",
    "best_comb, best_result = find_best_lambda(param_combinations, Y, X, k, n, Time, param, bounds)\n",
    "\n",
    "endtime = pd.Timestamp.now()\n",
    "print(\"Full Parameter Estimation has ended in \", endtime - starttime)\n",
    "print(\"Best lambda combination: \", best_comb)\n",
    "\n",
    "# Extract the optimized parameter values\n",
    "parameters_opt = best_result.x\n",
    "np.savetxt('optimised_parameterX.txt', parameters_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07487836",
   "metadata": {},
   "source": [
    "# Betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d229d42",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'parameters_opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#load the parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#parameters_opt = np.loadtxt('optimised_parameter.txt') # All data\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Extract the betas\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m Estimated_beta \u001b[38;5;241m=\u001b[39m parameters_opt[:k]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe estimated values of beta are\u001b[39m\u001b[38;5;124m\"\u001b[39m, Estimated_beta)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m############################################################################\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Define labels for the predictors\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parameters_opt' is not defined"
     ]
    }
   ],
   "source": [
    "#load the parameters\n",
    "#parameters_opt = np.loadtxt('optimised_parameter.txt') # All data\n",
    "\n",
    "#Extract the betas\n",
    "Estimated_beta = parameters_opt[:k]\n",
    "print(\"The estimated values of beta are\", Estimated_beta)\n",
    "############################################################################\n",
    "############################################################################\n",
    "\n",
    "# Define labels for the predictors\n",
    "predictor_labels = ['Constant', 'Trend', 'Sine_Y','Sine_SY','Sine_M','Sine_D','Cosine_Y','Cosine_SY','Cosine_M','Cosine_D']\n",
    "\n",
    "# Plotting the bar graph\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "bars = plt.bar(np.arange(len(Estimated_beta)), Estimated_beta, color='skyblue')\n",
    "plt.xlabel('Estimated Beta coefficients')\n",
    "plt.ylabel('Beta Value')\n",
    "plt.title('Estimated Beta coefficients')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.xticks(np.arange(len(predictor_labels)), predictor_labels)  # Set x-ticks to predictor labels\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee77e733",
   "metadata": {},
   "source": [
    "# Temporal dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da866c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "Estimated_phi = np.round(parameters_opt[k:k+n], 2)\n",
    "Locations = data.columns[1:]\n",
    "locdata = pd.read_excel('cordinates1.xlsx')\n",
    "table = pd.DataFrame({\"Index\": list(range(1,len(Locations)+1)), \"Location\": Locations,  \"Phi\": Estimated_phi})\n",
    "table[\"Longitude\"] = locdata['Longitude']\n",
    "table[\"Latitude\"] = locdata['Latitude']\n",
    "table[\"Phi\"] = table[\"Phi\"].apply(lambda x: \"{:.2f}\".format(x))\n",
    "table.to_csv(\"Location_T-Depedency.csv\", index=False)\n",
    "\n",
    "def plot_temporal_dependency(Estimated_phi, Locations):\n",
    "    from matplotlib import gridspec\n",
    "    # Create a 1x4 grid\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    gs = gridspec.GridSpec(1, 4, width_ratios=[1, 3, 3, 3])\n",
    "\n",
    "    # Create a subplot for the graph\n",
    "    ax_graph = plt.subplot(gs[1:4])\n",
    "\n",
    "    # Plot the data as bars\n",
    "    bars = ax_graph.bar(np.arange(len(Estimated_phi)), Estimated_phi)\n",
    "\n",
    "    # Set labels and title for the graph with bigger font size\n",
    "    ax_graph.set_xlabel('Location', fontsize=14)\n",
    "    ax_graph.set_ylabel('Dependency', fontsize=14)\n",
    "    ax_graph.set_title('Temporal Dependency \\n', fontsize=16)\n",
    "\n",
    "    # Add value annotations on top of each bar\n",
    "    for i, bar in enumerate(bars):\n",
    "        height = bar.get_height()\n",
    "        ax_graph.text(bar.get_x() + bar.get_width() / 2, height,\n",
    "                      f'{height:.2f}', ha='center', va='bottom', rotation=90, fontsize=10)\n",
    "\n",
    "    # Set x-axis tick labels as 1, 2, 3, ... with bigger font size\n",
    "    ax_graph.set_xticks(np.arange(len(Estimated_phi)))\n",
    "    ax_graph.set_xticklabels(np.arange(1, len(Estimated_phi) + 1), fontsize=12)\n",
    "\n",
    "    # Rotate x-axis tick labels\n",
    "    ax_graph.set_xticklabels(ax_graph.get_xticklabels(), rotation=45)\n",
    "\n",
    "    # Create a list of locations with indexes on the side of the graph\n",
    "    locations_text = '\\n'.join([f\"{i+1}. {loc}\" for i, loc in enumerate(Locations)])\n",
    "    # Add a title to the list of locations\n",
    "    locations_title = r\"$\\bf{Bavaria\\ Air\\ Quality\\ Measurement\\ Stations}$\"\n",
    "    locations_text = f\"{locations_title}\\n{locations_text}\"\n",
    "\n",
    "    ax_graph.text(1.06, 0.5, locations_text, transform=ax_graph.transAxes, fontsize=12, verticalalignment='center')\n",
    "\n",
    "    # Save the plot as a pdf\n",
    "    plt.savefig(\"temporal_with_location_list.pdf\", bbox_inches='tight')\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "#Plot your temporal dependency plot\n",
    "plot_temporal_dependency(Estimated_phi, Locations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc2135b",
   "metadata": {},
   "source": [
    "# Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb467b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.zeros((n, n))\n",
    "W[np.triu_indices(n, 1)] = parameters_opt[int(k+n):int(k+n+0.5*n*(n-1))]\n",
    "W[np.tril_indices(n, -1)] = parameters_opt[int(k+n+0.5*n*(n-1)):int(k+n*n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c13c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_estimated_weight_matrix(parameters_opt, n, save_filename=\"Estimated_Weights_Matrix.pdf\"):\n",
    "    # Estimated Weight Matrix\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, 1)] = parameters_opt[int(k+n):int(k+n+0.5*n*(n-1))]\n",
    "    W[np.tril_indices(n, -1)] = parameters_opt[int(k+n+0.5*n*(n-1)):int(k+n*n)]\n",
    "\n",
    "    # Define the custom colormap\n",
    "    colors = [(1, 1, 1), (0.3, 0.3, 0.3), (0.2, 0.2, 0.2), (0.8, 0.2, 0.2), (1, 0, 0)]\n",
    "    cmap_gray = mcolors.LinearSegmentedColormap.from_list('custom_gray', colors)\n",
    "\n",
    "    # Create the figure and axes\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "\n",
    "    # Set the title and plot the image\n",
    "    title = \"Estimated Spatial Weights\"\n",
    "    im = ax.imshow(W, cmap=cmap_gray, origin='upper')\n",
    "\n",
    "    # Set axis properties\n",
    "    ax.set_title(title)\n",
    "    x_ticks = np.arange(0, n, 2)  # Specify the positions for x-axis tick labels at intervals of 3\n",
    "    y_ticks = np.arange(0, n, 2)  # Specify the positions for y-axis tick labels at intervals of 3\n",
    "    ax.set_xticks(x_ticks)\n",
    "    ax.set_yticks(y_ticks)\n",
    "    x_labels = np.arange(1, n + 1, 2)  # Generate labels to start from 1 at intervals of 3\n",
    "    y_labels = np.arange(1, n + 1, 2)  # Generate labels to start from 1 at intervals of 3\n",
    "    ax.set_xticklabels(x_labels)\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    cbar = plt.colorbar(im, ax=ax, orientation='horizontal', fraction=0.04)\n",
    "\n",
    "    # Save the figure as an image\n",
    "    plt.savefig(save_filename)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "#Replace parameters_opt with your actual data and set n to the desired number of spatial units.\n",
    "plot_estimated_weight_matrix(parameters_opt, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8ca36f",
   "metadata": {},
   "source": [
    "# NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.read_csv('Location_T-Depedency.csv')\n",
    "def plot_network(W, coords, filename):\n",
    "    # Create an igraph Graph object\n",
    "    graph = ig.Graph.Weighted_Adjacency(W.tolist(), mode=\"directed\")\n",
    "\n",
    "    # Assign node coordinates and labels\n",
    "    graph.vs[\"name\"] = coords['Location']\n",
    "    graph.vs[\"x\"] = coords['Longitude']\n",
    "    graph.vs[\"y\"] = coords['Latitude']\n",
    "    node_labels = coords['Index']\n",
    "    graph.vs[\"label\"] = node_labels\n",
    "\n",
    "    # Set up visual style\n",
    "    visual_style = dict()\n",
    "    visual_style[\"vertex_size\"] = 20\n",
    "    visual_style[\"vertex_color\"] = \"lightblue\"\n",
    "    visual_style[\"edge_width\"] = 0.5\n",
    "    visual_style[\"edge_color\"] = \"gray\"\n",
    "\n",
    "    # Perform force-directed layout\n",
    "    layout = graph.layout_fruchterman_reingold(weights='weight')\n",
    "\n",
    "    # Adjust size of the graph layout to fit the page\n",
    "    width = 800  # Adjust this value according to your preference\n",
    "    height = 600  # Adjust this value according to your preference\n",
    "    layout.fit_into((width, height))\n",
    "\n",
    "    # Plot the graph and save as PDF\n",
    "    ig.plot(graph, filename, layout=layout, vertex_label=graph.vs[\"label\"], **visual_style)\n",
    "    ig.plot(graph, layout=layout, vertex_label=graph.vs[\"label\"], **visual_style)\n",
    "    plt.title(\"Network\")\n",
    "    plt.show()\n",
    "# Example usage:\n",
    "plot_network(W, coords, \"network_graph.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2956f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_network(W, coords, filename=\"network_graph.pdf\"):\n",
    "    # Create an igraph Graph object\n",
    "    graph = ig.Graph.Weighted_Adjacency(W.tolist(), mode=\"directed\")\n",
    "\n",
    "    # Assign node coordinates and labels\n",
    "    graph.vs[\"name\"] = coords['Location']\n",
    "    graph.vs[\"x\"] = coords['Longitude']\n",
    "    graph.vs[\"y\"] = coords['Latitude']\n",
    "    node_labels = coords['Index']\n",
    "    graph.vs[\"label\"] = node_labels\n",
    "\n",
    "    # Set up visual style\n",
    "    visual_style = dict()\n",
    "    visual_style[\"vertex_size\"] = 20\n",
    "    visual_style[\"vertex_color\"] = \"lightblue\"\n",
    "    visual_style[\"edge_width\"] = 0.5\n",
    "    visual_style[\"edge_color\"] = \"gray\"\n",
    "\n",
    "    # Perform force-directed layout\n",
    "    layout = graph.layout_fruchterman_reingold(weights='weight')\n",
    "\n",
    "    # Adjust size of the graph layout to fit the page\n",
    "    width = 800  # Adjust this value according to your preference\n",
    "    height = 600  # Adjust this value according to your preference\n",
    "    layout.fit_into((width, height))\n",
    "\n",
    "    # Plot the graph and save as PDF\n",
    "    #ig.plot(graph, filename, layout=layout, vertex_label=graph.vs[\"label\"], **visual_style)\n",
    "    ig.plot(graph, layout=layout, vertex_label=graph.vs[\"label\"], **visual_style)\n",
    "    plt.title(\"Network\")\n",
    "    plt.show()\n",
    "# Example usage:\n",
    "plot_network(W, coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad68a6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weights_matrix_and_network(parameters_opt, n, save_filename=\"Weights_Matrix_and_Network.pdf\"):\n",
    "    # Estimated Weight Matrix\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, 1)] = parameters_opt[int(k+n):int(k+n+0.5*n*(n-1))]\n",
    "    W[np.tril_indices(n, -1)] = parameters_opt[int(k+n+0.5*n*(n-1)):int(k+n*n)]\n",
    "\n",
    "       # Define the custom colormap\n",
    "    colors = [(1, 1, 1), (0.3, 0.3, 0.3), (0.2, 0.2, 0.2), (0.8, 0.2, 0.2), (1, 0, 0)]\n",
    "    cmap_gray = mcolors.LinearSegmentedColormap.from_list('custom_gray', colors)\n",
    "\n",
    "    # Create the figure and axes for subplots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Plot the weights matrix\n",
    "    ax1 = axes[0]\n",
    "    im = ax1.imshow(W, cmap=cmap_gray, origin='upper')\n",
    "    ax1.set_title(\"Estimated Weight Matrix\")\n",
    "    ax1.set_xlabel(\"Node Index\")\n",
    "    ax1.set_ylabel(\"Node Index\")\n",
    "    cbar = plt.colorbar(im, ax=ax1, orientation='vertical', fraction=0.05)\n",
    "\n",
    "    # Estimated Weight Matrix\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, 1)] = parameters_opt[int(n):int(n+0.5*n*(n-1))]\n",
    "    W[np.tril_indices(n, -1)] = parameters_opt[int(n+0.5*n*(n-1)):int(n*n)]\n",
    "\n",
    "    # Create an igraph Graph object\n",
    "    graph = ig.Graph.Weighted_Adjacency(W.tolist(), mode=\"directed\")\n",
    "\n",
    "    # Set up visual style for the network graph\n",
    "    visual_style_network = dict()\n",
    "    visual_style_network[\"vertex_size\"] = 20\n",
    "    visual_style_network[\"vertex_color\"] = \"lightblue\"\n",
    "    visual_style_network[\"edge_width\"] = 0.5\n",
    "    visual_style_network[\"edge_color\"] = [(0.3, 0.3, 0.3, weight) for weight in graph.es[\"weight\"]]\n",
    "\n",
    "    # Perform force-directed layout for the network graph\n",
    "    layout_network = graph.layout_fruchterman_reingold(weights='weight')\n",
    "\n",
    "    # Plot the network graph\n",
    "    ax2 = axes[1]\n",
    "    ig.plot(graph, layout=layout_network, ax=ax2, **visual_style_network)\n",
    "    ax2.set_title(\"Network\")\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save the figure as a PDF\n",
    "    plt.savefig(save_filename)\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Example usage:\n",
    "plot_weights_matrix_and_network(parameters_opt, n, \"weights_matrix_and_network.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663d9e3",
   "metadata": {},
   "source": [
    "# Predicted values and residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a84675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_predicted_values(parameters, Y, X, k, n, Time):\n",
    "    beta = parameters[:k]\n",
    "    phi = parameters[k:k+n]\n",
    "    W = np.zeros((n, n))\n",
    "    W[np.triu_indices(n, 1)] = parameters[int(k+n):int(k+n+0.5*n*(n-1))]\n",
    "    W[np.tril_indices(n, -1)] = parameters[int(k+n+0.5*n*(n-1)):int(k+n*n)]\n",
    "    \n",
    "    Y_pred = np.zeros_like(Y)\n",
    "    \n",
    "    for t in range(1, Time):\n",
    "        Y_pred[:, t] = W @ Y[:, t] + (phi * Y[:, t-1]) + X[:, t] @ beta\n",
    "    \n",
    "    return Y_pred\n",
    "Y_pred = compute_predicted_values(parameters_opt, Y, X, k, n, Time)\n",
    "#########################################################\n",
    "####################Compute residuals\n",
    "residuals = Y - Y_pred\n",
    "dates = pd.to_datetime(data['Date'])\n",
    "# Plot residuals over time\n",
    "plt.plot(dates, residuals.T)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals plot')\n",
    "plt.show()\n",
    "#plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
    "#plt.tight_layout() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119b024a",
   "metadata": {},
   "source": [
    "# Outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa678d",
   "metadata": {},
   "source": [
    "# Autocorrelation plots of residuals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983cd16a",
   "metadata": {},
   "source": [
    "# ACF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.graphics.tsaplots as tsaplots\n",
    "from scipy.stats import zscore\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate residuals\n",
    "residuals = Y - Y_pred\n",
    "\n",
    "# Check for outliers\n",
    "outliers = np.abs(zscore(residuals)) > 3  # Adjust the threshold as needed\n",
    "\n",
    "# Check for constant mean and variance\n",
    "mean_residuals = np.mean(residuals, axis=1)\n",
    "var_residuals = np.var(residuals, axis=1)\n",
    "\n",
    "# Assuming n is the number of stations\n",
    "# Plot ACF functions grid with 4 columns\n",
    "num_columns = 4\n",
    "num_rows = -(-n // num_columns)  # Ceiling division to determine the number of rows\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(20, 3*num_rows))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < n:  # Check if the current station index is within the total number of stations\n",
    "        # ACF plot with confidence intervals\n",
    "        tsaplots.plot_acf(residuals[i, :], lags=30, alpha=0.05, ax=ax)  # alpha=0.05 for 95% CI\n",
    "        ax.set_title(f'Residuals ACF - Station {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b86b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from scipy.stats import zscore\n",
    "# Calculate residuals\n",
    "residuals = Y - Y_pred\n",
    "\n",
    "#print(Y_pred)\n",
    "# Check for outliers\n",
    "outliers = np.abs(zscore(residuals)) > 3  # Adjust the threshold as needed\n",
    "\n",
    "# Check for constant mean and variance\n",
    "mean_residuals = np.mean(residuals, axis=1)\n",
    "var_residuals = np.var(residuals, axis=1)\n",
    "\n",
    "# Assuming n is the number of stations\n",
    "# Plot ACF functions grid with 4 columns\n",
    "num_columns = 4\n",
    "num_rows = -(-n // num_columns)  # Ceiling division to determine the number of rows\n",
    "\n",
    "fig, axes = plt.subplots(nrows=num_rows, ncols=num_columns, figsize=(20, 3*num_rows))\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < n:  # Check if the current station index is within the total number of stations\n",
    "        # ACF plot\n",
    "        sm.graphics.tsa.plot_acf(residuals[i, :], lags=30, ax=ax)\n",
    "        ax.set_title(f'Residuals ACF - Station {i+1}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a88a24",
   "metadata": {},
   "source": [
    "# Station ACF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25dc5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_station_acf(residuals, station_index, max_lags=50, alpha=0.05):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    sm.graphics.tsa.plot_acf(residuals[station_index-1, :], ax=ax, lags=max_lags, alpha=alpha)\n",
    "    ax.set_title(f'ACF - Station {station_index}')\n",
    "    plt.show()\n",
    "\n",
    "plot_station_acf(residuals, 26)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115521a8",
   "metadata": {},
   "source": [
    "# Fourier Splines and Original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03eed12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_vs_fourier(Y, Y_pred):\n",
    "    # Plot the original data and the predicted values (Fourier splines)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(dates, np.median(Y, axis=0), label='Original Data', color='grey')\n",
    "    plt.plot(dates, np.median(Y_pred,  axis=0), label='predicted', color='red')\n",
    "\n",
    "    # Customize the plot\n",
    "    plt.xlabel('hourly time points')\n",
    "    plt.ylabel('Median PM Concetration levels across stations')\n",
    "    plt.title('Fourier Splines and data')\n",
    "    plt.yscale('log')  # Set log scale for y-axis\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Customize legend with handles\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc='upper right', fontsize='medium')\n",
    "\n",
    "\n",
    "    # Save the plot as a PDF\n",
    "    plt.savefig(\"Fourier splines\")\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "plot_original_vs_fourier(Y, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a76f2",
   "metadata": {},
   "source": [
    "# Comparison with other models\n",
    "## Autoregressive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b4b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the cleaned data\n",
    "data = pd.read_csv('PM10_BFF.csv')\n",
    "Y = data.drop('Date', axis=1).values.T\n",
    "Y = Y[:, :50]\n",
    "#########################################\n",
    "Time = Y.shape[1]\n",
    "print('The number of hourly timepoints are',Time)\n",
    "n = Y.shape[0]\n",
    "print('The number of monitoring stations are',n)# Create an array to represent the time index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c398915a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the lag order (e.g., AR(1))\n",
    "lag_order = 1\n",
    "\n",
    "# Initialize lists to store AR coefficients for each station\n",
    "ar_coeffs = []\n",
    "\n",
    "# Estimate AR model for each station\n",
    "for i in range(Y.shape[0]):\n",
    "    ar_model = sm.tsa.AutoReg(Y[i, :], lags=lag_order)\n",
    "    ar_result = ar_model.fit()\n",
    "    ar_coeffs.append(ar_result.params)\n",
    "\n",
    "# Convert AR coefficients to DataFrame for easy inspection\n",
    "ar_coeffs_df = pd.DataFrame(ar_coeffs, index=Y.index)\n",
    "\n",
    "# Print AR coefficients\n",
    "print(ar_coeffs_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132f6852",
   "metadata": {},
   "source": [
    "# VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9babfea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Define the lag order (e.g., VAR(1))\n",
    "lag_order = 1\n",
    "\n",
    "# Initialize VAR model\n",
    "var_model = VAR(Y)\n",
    "\n",
    "# Fit VAR model\n",
    "var_result = var_model.fit(lag_order)\n",
    "\n",
    "# Print summary of VAR model\n",
    "print(var_result.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e839535",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
